Autonome Fahrzeuge müssen besonders während ihrer frühen Markteinführungsphase in der Lage sein, sich in homogenem Mischverkehr zurecht zu finden. Dabei reicht es möglicherweise nicht aus, sich lediglich auf die lokale Sensorik mit eingeschränkter Reichweite zu verlassen, um komplexe, innerstädtische Verkehrssituationen zuverlässig wahrzunehmen. Stattdessen können intelligente Fahrzeuge ihre perzeptuelle Sensorreichtweite durch den Einsatz von V2X Kommunikation und Cooperative Perception virtuell über ihren ursprünglichen Horizont hinaus erweitern. Bisherige Ansätze gehen jedoch mit einer Reihe von Einschränkungen einher, da sie oftmals wenig holistisch, nicht ausreichend allgemeingültig oder schlecht skalierbar sind. Daher hat diese Masterarbeit das Ziel, ein umfangreiches Cooperative Perception System aus Basis modernster Techniken zu entwerfen, implementieren und evaluieren. Dazu wird zunächst ein umfassender, aber dennoch leicht erweiterbarer Modellierungsansatz für dynamische Verkerhssituationen vorgestellt, der mithilfe probabilistischer Entity-Relationship Modelle ungewisse Umgebungenswahrnehmungen berücksichtigt und einfache Attribute mit abstrakteren, relationalen und semantischen Informationen auf möglichst generische Weise kombiniert. Anschließend wird der Entwurf einer ganzheitlichen, verteilten Software Architektur auf Basis von Mobile Edge Computing-Prinzipien als Grundlage für fahrzeugübergreifende high-level Sensorfusion vorgestellt. Im Gegensatz zu den meisten bestehenden Ansätzen setzt unsere Lösung auf Cellular-V2X Kommunikation in 5G-Netzen und verwendet geographisch verteilte Rechenknoten. Eine modulare Proof-of-Concept Implementierung wird in verschiedenen simulierten Szenarien evaluiert, um die Leistung des Systems sowohl qualitativ als auch quantitativ zu bewerten. Unsere Experimente zeigen schließlich, dass das vorgestellte System ausreichend gut skaliert, um bestimmte Mindesanforderungen zu erfüllen und erzielt eine durchschnittliche Verbesserung der Wahrnehmungsqualität von ca. 27 \%. 